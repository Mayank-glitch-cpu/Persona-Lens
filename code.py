# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jpxlNg3NIYWuy_etVN-hc8umOHG65n2g
"""

import pandas as pd

# Load the dataset
file_path = '/Users/chetan/Leetcode_DataExtraction/github_users_combined.csv'  # Replace with the actual path to your dataset
data = pd.read_csv(file_path)

# Get the column names
column_names = data.columns.tolist()

# Get the number of rows and columns
num_rows, num_columns = data.shape

# Print the results
print("Column Names:", column_names)
print("Number of Rows:", num_rows)
print("Number of Columns:", num_columns)

import pandas as pd

# Load the new dataset
file_path = '/Users/chetan/Leetcode_DataExtraction/github_users_combined.csv'  # Replace with the actual path to your dataset
data = pd.read_csv(file_path)

# 1. Define the Columns to Keep
required_columns = [
    'username', 'bio', 'profile_links', 'public_repos', 'total_stars', 'total_forks',
    'most_used_language', 'followers', 'following', 'contributions_last_year',
    'account_created', 'location', 'company', 'email', 'blog', 'language_stats'
]

# 2. Filter the Dataset to Keep Only the Required Columns
filtered_data = data[required_columns]

# 3. Save the Cleaned Dataset
output_file_path = 'cleaned_dataset.csv'  # Replace with your desired output file path
filtered_data.to_csv(output_file_path, index=False)

print("Cleaned dataset saved to:", output_file_path)
print("Columns in the cleaned dataset:", filtered_data.columns.tolist())
print("Number of rows in the cleaned dataset:", len(filtered_data))

import pandas as pd
from rake_nltk import Rake
from datetime import datetime

# Load the cleaned dataset
file_path = 'cleaned_dataset.csv'  # Replace with the actual path to your cleaned dataset
data = pd.read_csv(file_path)

# 1. Calculate Experience Level
def calculate_experience(account_created):
    if pd.isna(account_created):
        return 0
    created_date = datetime.strptime(account_created, '%Y-%m-%d')
    today = datetime.today()
    return (today - created_date).days // 365

data['experience_years'] = data['account_created'].apply(calculate_experience)

# 2. Extract Skill Keywords from Bio
def extract_skills(bio):
    if pd.isna(bio):
        return []
    rake = Rake()
    rake.extract_keywords_from_text(bio)
    return rake.get_ranked_phrases()

data['skills'] = data['bio'].apply(extract_skills)

# 3. Create a Popularity Score
data['popularity_score'] = data['total_stars'] + data['total_forks']

# 4. Parse Language Stats (if needed)
# Assuming language_stats is a JSON string or dictionary
import ast
def parse_language_stats(language_stats):
    if pd.isna(language_stats):
        return {}
    return ast.literal_eval(language_stats)

data['language_stats_parsed'] = data['language_stats'].apply(parse_language_stats)

# Save the dataset with extracted features
output_file_path = 'dataset_with_extracted_features.csv'  # Replace with your desired output file path
data.to_csv(output_file_path, index=False)

print("Dataset with extracted features saved to:", output_file_path)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset with extracted features
file_path = 'dataset_with_extracted_features.csv'  # Replace with the actual path to your dataset
data = pd.read_csv(file_path)

# 1. Distribution of Experience Years
plt.figure(figsize=(10, 6))
sns.histplot(data['experience_years'], bins=20, kde=True)
plt.title('Distribution of Experience Years')
plt.xlabel('Experience Years')
plt.ylabel('Frequency')
plt.show()

# 2. Distribution of Popularity Score
plt.figure(figsize=(10, 6))
sns.histplot(data['popularity_score'], bins=20, kde=True)
plt.title('Distribution of Popularity Score')
plt.xlabel('Popularity Score')
plt.ylabel('Frequency')
plt.show()

# 3. Correlation Heatmap
plt.figure(figsize=(10, 6))
corr = data[['public_repos', 'total_stars', 'total_forks', 'followers', 'following', 'contributions_last_year', 'experience_years', 'popularity_score']].corr()
sns.heatmap(corr, annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

# #Feature Fusion:
# Combine the extracted features into a single feature set for each user. For example:

# Numerical Features:

# public_repos

# total_stars

# total_forks

# followers

# following

# contributions_last_year

# experience_years

# popularity_score

# Categorical Features:

# most_used_language

# primary_language (from language_stats_parsed)

# Textual Features:

# skills (extracted from bio)

# bio (raw text for embedding)

# Step 2: Persona Modeling:
# Use clustering or classification algorithms to group users into personas. Hereâ€™s how:

# Option 1: Clustering (Unsupervised Learning):
# Use algorithms like K-Means or DBSCAN to group users based on their features.

# Example personas:

# Open-Source Contributor: High public_repos, total_stars, total_forks.

# Competitive Programmer: High activity on LeetCode (if integrated later).

# Researcher: Strong presence on ResearchGate (if integrated later).


import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Load the dataset with extracted features
file_path = 'dataset_with_extracted_features.csv'  # Replace with the actual path to your dataset
data = pd.read_csv(file_path)

# Select numerical features for clustering
features = ['public_repos', 'total_stars', 'total_forks', 'followers', 'following', 'contributions_last_year', 'experience_years', 'popularity_score']
X = data[features]

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Perform K-Means clustering
kmeans = KMeans(n_clusters=3, random_state=42)  # Adjust n_clusters based on your needs
data['persona'] = kmeans.fit_predict(X_scaled)

# Visualize the clusters
plt.figure(figsize=(10, 6))
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=data['persona'], cmap='viridis', marker='o')
plt.title('K-Means Clustering of GitHub Users')
plt.xlabel('Feature 1 (e.g., public_repos)')
plt.ylabel('Feature 2 (e.g., total_stars)')
plt.colorbar(label='Persona')
plt.show()

# Save the dataset with personas
output_file_path = 'dataset_with_personas.csv'  # Replace with your desired output file path
data.to_csv(output_file_path, index=False)

print("Dataset with personas saved to:", output_file_path)

import pandas as pd

# Load the dataset with extracted features
file_path = '/Users/chetan/Leetcode_DataExtraction/dataset_with_personas.csv'  # Replace with the actual path to your dataset
data = pd.read_csv(file_path)

# 1. Filter numeric columns for aggregation
numeric_columns = data.select_dtypes(include=['number']).columns
print("Numeric Columns:", numeric_columns.tolist())

# 2. Compute mean values of numeric features for each cluster
cluster_summary = data.groupby('persona')[numeric_columns].mean()
print("Cluster Summary (Numeric Features):")
print(cluster_summary)

# 3. Analyze non-numeric columns separately
# Example: Count unique skills in each cluster
if 'skills' in data.columns:
    data['skills_count'] = data['skills'].apply(lambda x: len(x) if isinstance(x, list) else 0)
    skills_summary = data.groupby('persona')['skills_count'].mean()
    print("Average Number of Skills per Cluster:")
    print(skills_summary)

# Example: Extract most common primary language in each cluster
if 'most_used_language' in data.columns:
    language_summary = data.groupby('persona')['most_used_language'].agg(lambda x: x.mode()[0])
    print("Most Common Primary Language per Cluster:")
    print(language_summary)

# Assign persona labels based on cluster numbers
persona_labels = {
    0: 'JavaScript Developers',
    1: 'C++ Enthusiasts',
    2: 'Python Superstars'
}

data['persona_label'] = data['persona'].map(persona_labels)

# Save the dataset with persona labels
output_file_path = 'dataset_with_persona_labels.csv'  # Replace with your desired output file path
data.to_csv(output_file_path, index=False)

print("Dataset with persona labels saved to:", output_file_path)

import matplotlib.pyplot as plt
import seaborn as sns

# Scatter plot: total_stars vs. total_forks, colored by persona_label
plt.figure(figsize=(10, 6))
sns.scatterplot(x='total_stars', y='total_forks', hue='persona_label', data=data, palette='viridis')
plt.title('Clusters: Total Stars vs. Total Forks')
plt.xlabel('Total Stars')
plt.ylabel('Total Forks')
plt.legend(title='Persona')
plt.show()

import pandas as pd

# Load the dataset with persona labels
file_path = 'dataset_with_persona_labels.csv'  # Replace with the actual path to your dataset
data = pd.read_csv(file_path)

# Function to query candidates by persona
def query_candidates_by_persona(persona_label):
    return data[data['persona_label'] == persona_label]

# Example: Find all Python Superstars
python_superstars = query_candidates_by_persona('Python Superstars')
print("Python Superstars:")
print(python_superstars[['username', 'public_repos', 'total_stars', 'total_forks', 'followers']])

# Example: Find all JavaScript Developers
javascript_developers = query_candidates_by_persona('JavaScript Developers')
print("JavaScript Developers:")
print(javascript_developers[['username', 'public_repos', 'total_stars', 'total_forks', 'followers']])

# Example: Find all C++ Enthusiasts
cpp_enthusiasts = query_candidates_by_persona('C++ Enthusiasts')
print("C++ Enthusiasts:")
print(cpp_enthusiasts[['username', 'public_repos', 'total_stars', 'total_forks', 'followers']])

import matplotlib.pyplot as plt
import seaborn as sns

# Bar plot: Number of candidates per persona
plt.figure(figsize=(10, 6))
sns.countplot(x='persona_label', data=data, palette='viridis')
plt.title('Number of Candidates per Persona')
plt.xlabel('Persona')
plt.ylabel('Count')
plt.show()

